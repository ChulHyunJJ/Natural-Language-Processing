{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"조철현 - _13 트랜스포머 (Transformer).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6PQSnsGeA2OH"},"source":["# 트랜스포머 (Transformer)\n","\n","* 참고: https://wikidocs.net/31379"]},{"cell_type":"markdown","metadata":{"id":"nbQ-h_XxBAiq"},"source":["* attention mechanism은 seq2seq의 입력 시퀀스 정보 손실을 보정해주기 위해 사용됨\n","* attention mechanism을 보정 목적이 아닌, 인코더와 디코더로 구성한 모델이 바로 트랜스포머\n","* 트랜스포머는 RNN을 사용하지 않고 인코더와 디코더를 설계하였으며, 성능도 RNN보다 우수함\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RDiFPIdUBBS2"},"source":["## 포지셔널 인코딩"]},{"cell_type":"markdown","metadata":{"id":"rLqHf_4SEWoa"},"source":["* 기존의 RNN은 단어의 위치를 따라 순차적으로 입력받아 단어의 위치정보를 활용할 수 있었음\n","* 트랜스포머의 경우, RNN을 활용하지 않았기 때문에 단어의 위치정보를 다른 방식으로 줄 필요가 있음\n","* 이를 위해 **각 단어의 임베딩 벡터에 위치 정보들을 더하게 되는데** 이를 포지셔널 인코딩이라 함\n","* 보통 포지셔널 인코딩은 sin, cos을 이용하여 계산"]},{"cell_type":"code","metadata":{"id":"SiO5c_HIFBAk","executionInfo":{"status":"ok","timestamp":1604154254447,"user_tz":-540,"elapsed":1106,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["def positional_encoding(dim, sentence_length):\n","  encoded_vec = np.array([pos / np.power(10000, 2 * i / dim) for pos in range(sentence_length) for i in range(dim)])\n","  encoded_vec[::2] = np.sin(encoded_vec[::2])\n","  encoded_vec[1::2] = np.cos(encoded_vec[1::2])\n","  \n","  return tf.constant(encoded_vec.reshape([sentence_length, dim]), dtype=tf.float32)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"099gUUxhAgy3"},"source":["## 레이어 정규화"]},{"cell_type":"markdown","metadata":{"id":"XCdips98yPuH"},"source":["*  레이어 정규화에서는 텐서의 마지막 차원에 대해 평균과 분산을 구하고, 이 값을 통해 값을 정규화함\n","*  해당 정규화를 각 층의 연결에 편리하게 적용하기 위해 함수화한 `sublayer_connection()`을 선언"]},{"cell_type":"code","metadata":{"id":"TSJjxF86Aeg3","executionInfo":{"status":"ok","timestamp":1604154254764,"user_tz":-540,"elapsed":1403,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["def layer_norm(inputs, eps=1e-6):\n","  feature_shape = inputs.get_shape()[-1:]\n","  mean = tf.keras.backend.mean(inputs, [-1], keepdims=True)\n","  std = tf.keras.backend.std(inputs, [-1], keepdims=True)\n","  beta = tf.Variable(tf.zeros(feature_shape), trainable=False)\n","  gamma = tf.Variable(tf.ones(feature_shape), trainable=False)\n","  return gamma * (inputs - mean) / (std + eps) + beta"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"km9ORxIun-MU","executionInfo":{"status":"ok","timestamp":1604154254765,"user_tz":-540,"elapsed":1361,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["def sublayer_connection(inputs, sublayer, dropout=0.2):\n","  outputs = layer_norm(inputs + tf.keras.layers.Dropout(dropout)(sublayer))\n","  return outputs"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ppb7IxJ3diMC"},"source":["## 어텐션"]},{"cell_type":"markdown","metadata":{"id":"1JaU6MHgy9V2"},"source":["\n","\n","*   트랜스포머 모델의 핵심이 되는 부분\n","*   트랜스포머에서는 multi-head attention과 self attention이라는 개념을 사용\n","  1.   multi-head attention\n","      * 디코더가 가지는 차원을 나누어 병렬로 어텐션을 진행\n","      *  마지막엔 병렬로 각 진행해 얻은 어텐션 헤드를 모두 연결\n","      * 이로 인해 다양한 시각에서 정보를 수집할 수 있는 효과를 얻음\n","  2.   self attention\n","      *   일반적인 어텐션의 경우, 특정 시점의 디코더 은닉상태와 모든 시점의 인코더 은닉상태를 활용\n","      *   이는 입력 문장과 다른 문장에 존재하는 단어간의 어텐션을 의미함\n","      *   반면 self attention은 은닉 상태를 동일하게 하여 어텐션을 진행\n","      *   이는 입력 문장 내 단어간의 어텐션을 의미함\n","\n","\n","\n","\n","*   트랜스포머 제안 논문에서는 scaled-dot product attention을 활용해 모델을 작성함\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kRyL0KDXi6ej"},"source":["### scaled-dot product attention 구현"]},{"cell_type":"markdown","metadata":{"id":"6HtmcgRR3Cr-"},"source":["* scaled-dot product attention은 앞서 학습한 dot product attention과 거의 유사함\n","* 단 attention을 진행할 때 어텐션 스코어를 계산할 때 내적 값을 정규화\n","* 트랜스포머에서는 정규화할 때 K 벡터(=디코더 셀의 은닉 상태)의 차원을 루트를 취한 값을 사용"]},{"cell_type":"code","metadata":{"id":"ALEMzi4fdiSQ","executionInfo":{"status":"ok","timestamp":1604154254766,"user_tz":-540,"elapsed":1352,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["def scaled_dot_product_attention(query, key, value, masked=False):\n","  key_dim_size = float(key.get_shape().as_list()[-1])\n","  key = tf.transpose(key, perm=[0, 2, 1])\n","\n","  outputs = tf.matmul(query, key) / tf.sqrt(key_dim_size)\n","\n","  if masked:\n","    diag_vals = tf.ones_like(outputs[0, :, :])\n","    tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()\n","    masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])\n","    paddings = tf.ones_like(masks) * (-2**30)\n","    outputs = tf.where(tf.equal(masks, 0), paddings, outputs)\n","\n","  attention_map = tf.nn.softmax(outputs)\n","  return tf.matmul(attention_map, value)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yr20BxvVi-8b"},"source":["### multi-head attention 구현"]},{"cell_type":"markdown","metadata":{"id":"Gb5qflUH14-H"},"source":["* multi-head attention의 구현 과정\n","  1. query, key, value에 해당하는 값을 받고, 해당 값에 해당하는 행렬 생성\n","  2. 생성된 행렬들을 heads에 해당하는 수만큼 분리\n","  3. 분리한 행렬들에 대해 각각 어텐션을 수행\n","  4. 각 어텐션 결과들을 연결해 최종 어텐션 결과 생성\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ooc3FAdQi_Gz","executionInfo":{"status":"ok","timestamp":1604154254766,"user_tz":-540,"elapsed":1342,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["def multi_head_attention(query, key, value, num_units, heads, masked=False):\n","  query = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(query)\n","  key = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(key)\n","  value = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(value)\n","  \n","  query = tf.concat(tf.split(query, heads, axis=-1), axis=0)\n","  key = tf.concat(tf.split(key, heads, axis=-1), axis=0)\n","  value = tf.concat(tf.split(value, heads, axis=-1), axis=0)\n","  \n","  attention_map = scaled_dot_product_attention(query, key, value, masked)\n","  attn_outputs = tf.concat(tf.split(attention_map, heads, axis=0), axis=-1)\n","  attn_outputs = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(attn_outputs)\n","\n","  return attn_outputs"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"78Zn5-fYITD4"},"source":["## 포지션-와이즈 피드 포워드 신경망"]},{"cell_type":"markdown","metadata":{"id":"-xxeG2xvo3ZN"},"source":["\n","\n","*   multi-head attention의 결과인 행렬을 입력받아 연산\n","*   일반적인 완전 연결 신경망(Dense layer)를 사용\n","*   position-wise FFNN은 인코더와 디코더에 모두 존재\n","\n"]},{"cell_type":"code","metadata":{"id":"0tSFd5OaITJ0","executionInfo":{"status":"ok","timestamp":1604154254767,"user_tz":-540,"elapsed":1331,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["def feed_forward(inputs, num_units):\n","  feature_shape = inputs.get_shape()[-1]\n","  inner_layer = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(inputs)\n","  outputs = tf.keras.layers.Dense(feature_shape)(inner_layer)\n","  return outputs"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XuccViYgBK6v"},"source":["## 인코더\n"]},{"cell_type":"markdown","metadata":{"id":"tG3MH0n1JVLz"},"source":["* 인코더는 하나의 어텐션을 사용\n","  + encoder self-attention (multi-head self-attention과 동일)"]},{"cell_type":"code","metadata":{"id":"m5T0pzBoAnn3","executionInfo":{"status":"ok","timestamp":1604154254768,"user_tz":-540,"elapsed":1322,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["def encoder_module(inputs, model_dim, ffn_dim, heads):\n","  self_attn = sublayer_connection(inputs, multi_head_attention(inputs, inputs, inputs, model_dim, heads))\n","  outputs = sublayer_connection(self_attn, feed_forward(self_attn, ffn_dim))\n","  return outputs\n","\n","def encoder(inputs, model_dim, ffn_dim, heads, num_layers):\n","  outputs = inputs\n","  for i in range(num_layers):\n","    outputs = encoder_module(outputs, model_dim, ffn_dim, heads)\n","  \n","  return outputs"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lcgHRcTEBQqg"},"source":["## 디코더"]},{"cell_type":"markdown","metadata":{"id":"cNj-6FLQwT4-"},"source":["* 디코더는 다음과 같은 구성의 반복으로 이루어짐\n","  1. masked decoder self-attention\n","  2. encoder-decoder attention\n","  3. position-wise FFNN\n","\n","* 디코더에서는 2종류의 어텐션을 사용\n","  1.   masked decoder self-attention\n","    *   디코더에서는 인코더와는 달리 순차적으로 결과를 만들어 내야하기 때문에 다른 어텐션 방법을 사용함\n","    *   디코더 예측 시점 이후의 위치에 attention을 할 수 없도록 masking 처리\n","    *   결국 예측 시점에서 예측은 미리 알고 있는 위치까지만의 결과에 의존\n","  2.   encoder-decoder attention\n","    *   앞서 설명한 multi-head attention과 동일\n","\n"]},{"cell_type":"code","metadata":{"id":"2B05wr7aARcT","executionInfo":{"status":"ok","timestamp":1604154254770,"user_tz":-540,"elapsed":1314,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["def decoder_module(inputs, encoder_outputs, model_dim, ffn_dim, heads):\n","  masked_self_attn = sublayer_connection(inputs, \n","                                         multi_head_attention(inputs, inputs, inputs, \n","                                                              model_dim, heads, masked=True))\n","  self_attn = sublayer_connection(masked_self_attn, \n","                                  multi_head_attention(masked_self_attn, \n","                                                       encoder_outputs, encoder_outputs, model_dim, heads))\n","  \n","  outputs = sublayer_connection(self_attn, feed_forward(self_attn, ffn_dim))\n","  return outputs\n","\n","def decoder(inputs, encoder_outputs, model_dim, ffn_dim, heads, num_layers):\n","  outputs = inputs\n","  for i in range(num_layers):\n","    outputs = decoder_module(outputs, encoder_outputs, model_dim, ffn_dim, heads)\n","  \n","  return outputs"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EtztlyUB1ERS"},"source":["## 트랜스포머를 활용한 챗봇"]},{"cell_type":"markdown","metadata":{"id":"6CGUIAzv6eWs"},"source":["### konlpy 라이브러리"]},{"cell_type":"markdown","metadata":{"id":"Ae0mHT49v5gy"},"source":["*    한글을 처리하기 위해 konlpy 라이브러리 설치"]},{"cell_type":"code","metadata":{"id":"U8yf75uG6hBW","executionInfo":{"status":"ok","timestamp":1604154257452,"user_tz":-540,"elapsed":3985,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}},"outputId":"3bd9b36a-8d09-4459-fc03-1f913bcc3542","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install konlpy"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.1.2)\n","Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.9.0)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.4)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n","Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rUMXvK5H1G9H"},"source":["### 데이터 준비"]},{"cell_type":"markdown","metadata":{"id":"miXrjR316mNb"},"source":["* 처리에 필요한 각종 변수 선언\n","* filters에 해당되는 문자를 걸러주는 정규 표현식 컴파일\n","\n"]},{"cell_type":"code","metadata":{"id":"SMjn5PfE1GZR","executionInfo":{"status":"ok","timestamp":1604154259656,"user_tz":-540,"elapsed":6157,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["import re\n","import tensorflow as tf\n","\n","filters = \"([~.,!?\\\"':;)(])\"\n","PAD = '<PADDING>'\n","STD = '<START>'\n","END = '<END>'\n","UNK = '<UNKOWN>'\n","\n","PAD_INDEX = 0\n","STD_INDEX = 1\n","END_INDEX = 2\n","UNK_INDEX = 3\n","\n","MARKER = [PAD, STD, END, UNK]\n","CHANGE_FILTER = re.compile(filters)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xmRFuH2r6oNJ"},"source":["* 주소에서 데이터를 가져오는 `load_data()` 함수 선언\n","\n"]},{"cell_type":"code","metadata":{"id":"CmrmdXkePWYb","executionInfo":{"status":"ok","timestamp":1604154259668,"user_tz":-540,"elapsed":6160,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","def load_data(data_path):\n","  data_df = pd.read_csv(data_path, header=0)\n","  question, answer = list(data_df['Q']), list(data_df['A'])\n","  train_input, eval_input, train_label, eval_label = train_test_split(question, answer, test_size=0.33, random_state=111)\n","  \n","  return train_input, train_label, eval_input, eval_label"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vHuOJHPtPXqq"},"source":["* 처리에 필요한 단어 사전을 생성하는 `load_vocab()` 함수 선언"]},{"cell_type":"code","metadata":{"id":"QtQL-AP06oSa","executionInfo":{"status":"ok","timestamp":1604154259669,"user_tz":-540,"elapsed":6151,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["def load_vocabulary(data_path):\n","  data_df = pd.read_csv(data_path, encoding='utf-8')\n","  question, answer = list(data_df['Q']), list(data_df['A'])\n","  if tokenize_as_morph:\n","    question = prepro_like_morphlized(question)\n","    answer = prepro_like_morphlized(answer)\n","  \n","  data = []\n","  data.extend(question)\n","  data.extend(answer)\n","  words = data_tokenizer(data)\n","  words = list(set(words))\n","  words[:0] = MARKER\n","\n","  char2idx = {char:idx for idx, char in enumerate(words)}\n","  idx2char = {idx:char for idx, char in enumerate(words)}\n","  return char2idx, idx2char, len(char2idx)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5wYtpjv76r5q"},"source":["* 문자열 데이터를 학습에 사용될 수 있도록 변현하는 `prepro_like_morphlized()` 함수 선언\n","\n"]},{"cell_type":"code","metadata":{"id":"-bQ3FOva6tg6","executionInfo":{"status":"ok","timestamp":1604154259670,"user_tz":-540,"elapsed":6143,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["from konlpy.tag import Okt\n","\n","def prepro_like_morphlized(data):\n","  morph_analyzer = Okt()\n","  result_data = list()\n","\n","  for seq in data:\n","    morphlized_seq = ' '.join(morph_analyzer.morphs(seq.replace(' ', '')))\n","    result_data.append(morphlized_seq)\n","  \n","  return result_data"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vhsVp4pWPTR3"},"source":["* 단어 사전을 만들기 위해 단어들을 분리하는 `data_tokenizer()` 함수 선언"]},{"cell_type":"code","metadata":{"id":"otLI_RUfPR_g","executionInfo":{"status":"ok","timestamp":1604154259670,"user_tz":-540,"elapsed":6132,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["def data_tokenizer(data):\n","  words = []\n","  for sentence in data:\n","    sentence = re.sub(CHANGE_FILTER, '', sentence)\n","    for word in sentence.split():\n","      words.append(word)\n","  \n","  return [word for word in words if word]"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OkKPA-Mx6uaC"},"source":["* encoder의 입력을 구성하기 위한 함수 `enc_processing()` 선언\n","\n"]},{"cell_type":"code","metadata":{"id":"jK-yeSThPGsa","executionInfo":{"status":"ok","timestamp":1604154259671,"user_tz":-540,"elapsed":6120,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["def enc_processing(value, dictionary):\n","  sequences_input_index = []\n","  sequences_length = []\n","\n","  if tokenize_as_morph:\n","    value = prepro_like_morphlized(value)\n","  \n","  for sequence in value:\n","    sequence = re.sub(CHANGE_FILTER, '', sequence)\n","    sequence_index = []\n","    for word in sequence.split():\n","      if dictionary.get(word) is not None:\n","        sequence_index.extend([dictionary[word]])\n","      else:\n","        sequence_index.extend([dictionary[UNK]]) \n","    \n","    if len(sequence_index) > max_len:\n","      sequence_index = sequence_index[:max_len]\n","    sequences_length.append(len(sequence_index))\n","    sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n","    sequences_input_index.append(sequence_index)\n","  return np.asarray(sequences_input_index), sequences_length"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d4mM57_FPIg7"},"source":["* decoder의 입력을 구성하기 위한 함수 `dec_output_processing()` 선언"]},{"cell_type":"code","metadata":{"id":"cX_NpcTq6vw6","executionInfo":{"status":"ok","timestamp":1604154259671,"user_tz":-540,"elapsed":6106,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["def dec_output_processing(value, dictionary):\n","  sequences_output_index = []\n","  sequences_length = []\n","\n","  if tokenize_as_morph:\n","    value = prepro_like_morphlized(value)\n","  \n","  for sequence in value:\n","    sequence = re.sub(CHANGE_FILTER, '', sequence)\n","    sequence_index = []\n","    sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n","    \n","    if len(sequence_index) > max_len:\n","      sequence_index = sequence_index[:max_len]\n","    sequences_length.append(len(sequence_index))\n","    sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n","    sequences_output_index.append(sequence_index)\n","  return np.asarray(sequences_output_index), sequences_length"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otsTEt4FPLJX"},"source":["* decoder의 출력을 구성하기 위한 함수 `dec_target_processing()` 선언"]},{"cell_type":"code","metadata":{"id":"eeP0PWHEPMma","executionInfo":{"status":"ok","timestamp":1604154259672,"user_tz":-540,"elapsed":6096,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["def dec_target_processing(value, dictionary):\n","  sequences_target_index = []\n","\n","  if tokenize_as_morph:\n","    value = prepro_like_morphlized(value)\n","  \n","  for sequence in value:\n","    sequence = re.sub(CHANGE_FILTER, '', sequence)\n","    sequence_index = []\n","    sequence_index = [dictionary[word] for word in sequence.split()]\n","    \n","    if len(sequence_index) >= max_len:\n","      sequence_index = sequence_index[:max_len - 1] + [dictionary[END]]\n","    else:\n","      sequence_index += [dictionary[END]]\n","\n","    sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n","    sequences_target_index.append(sequence_index)\n","  return np.asarray(sequences_target_index)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tb9vVUng6xDq"},"source":["* 모델에 데이터를 효율적으로 투입하도록 `train_input_fn()`, `eval_input_fn()` 함수 선언\n","* `rearrange()`는 dataset 객체가 데이터를 어떻게 변형시킬지 정의해둔 함수\n","* dataset.map은 rearrange 함수를 기반으로 데이터를 변형\n","\n"]},{"cell_type":"code","metadata":{"id":"uAlKV4xF62Uf","executionInfo":{"status":"ok","timestamp":1604154259673,"user_tz":-540,"elapsed":6087,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["def train_input_fn(train_input_enc, train_output_enc, train_target_dec, batch_size):\n","  dataset = tf.compat.v1.data.Dataset.from_tensor_slices((train_input_enc, train_output_enc, train_target_dec))\n","  dataset = dataset.shuffle(buffer_size = len(train_input_enc))\n","  dataset = dataset.batch(batch_size)\n","  dataset = dataset.map(rearange)\n","  dataset = dataset.repeat()\n","  iterator = dataset.make_one_shot_iterator()\n","  return iterator.get_next()\n","\n","def eval_input_fn(eval_input_enc, eval_output_enc, eval_target_dec, batch_size):\n","  dataset = tf.compat.v1.data.Dataset.from_tensor_slices((eval_input_enc, eval_output_enc, eval_target_dec))\n","  dataset = dataset.shuffle(buffer_size = len(eval_input_enc))\n","  dataset = dataset.batch(batch_size)\n","  dataset = dataset.map(rearange)\n","  dataset = dataset.repeat(1)\n","  iterator = dataset.make_one_shot_iterator()\n","  return iterator.get_next()\n","\n","def rearange(input, output, target):\n","  features = {'input':input, 'output':output}\n","  return features, target"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"is-GhUDN62xC"},"source":["* 모델의 예측은 배열로 생성되기 때문에 이를 확인하기 위해선 문자열로 변환이 필요\n","* 예측을 문자열로 변환해주는 `pred2string()` 함수 선언\n"]},{"cell_type":"code","metadata":{"id":"jCfwWXhb64Cc","executionInfo":{"status":"ok","timestamp":1604154259674,"user_tz":-540,"elapsed":6078,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["def pred2string(value, dictionary):\n","  sentence_string = []\n","  is_finished = False\n","\n","  for v in value:\n","    sentence_string = [dictionary[index] for index in v['indexs']]\n","\n","  answer = ''\n","  for word in sentence_string:\n","    if word == END:\n","      is_finished = True\n","      break\n","    \n","    if word != PAD and word != END:\n","      answer += word\n","      answer += ' '\n","\n","  return answer, is_finished"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hwp9Nnwz7UoG"},"source":["* 챗봇 데이터 URL: https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv\n","* 데이터 주소에서 데이터를 읽어들여 단어 사전과 사용 데이터 구성"]},{"cell_type":"code","metadata":{"id":"-T536MdU7Taq","executionInfo":{"status":"ok","timestamp":1604154347287,"user_tz":-540,"elapsed":93681,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["import pandas as pd\n","\n","tokenize_as_morph = True\n","\n","data_path = 'https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv'\n","\n","char2idx, idx2char, len_vocab = load_vocabulary(data_path)\n","train_input, train_label, eval_input, eval_label = load_data(data_path)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7cVd7AOKinqn"},"source":["### 모델 구성"]},{"cell_type":"markdown","metadata":{"id":"hqLJ0a6r49yi"},"source":["* 앞서 작성한 트랜스포머 모델을 결합해 학습에 사용할 모델을 구성함"]},{"cell_type":"code","metadata":{"id":"CNeeXoZginvj","executionInfo":{"status":"ok","timestamp":1604154347291,"user_tz":-540,"elapsed":93674,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["def model(features, labels, mode, params):\n","  TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n","  EVAL = mode == tf.estimator.ModeKeys.EVAL\n","  PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n","  \n","  position_encode = positional_encoding(params['embedding_size'], params['max_len'])\n","  if params['xavier_initializer']:\n","    embedding_initializer = 'glorot_normal'\n","  else:\n","    embedding_initializer = 'uniform'\n","  \n","  embedding = tf.keras.layers.Embedding(params['len_vocab'], \n","                                        params['embedding_size'],\n","                                        embeddings_initializer=embedding_initializer)\n","  \n","  x_embedded_matrix = embedding(features['input']) + position_encode\n","  y_embedded_matrix = embedding(features['output']) + position_encode\n","\n","  encoder_outputs = encoder(x_embedded_matrix, params['model_hidden_size'], params['ffn_hidden_size'], \n","                            params['attention_head_size'], params['layer_size'])\n","  decoder_outputs = decoder(y_embedded_matrix, encoder_outputs, params['model_hidden_size'], \n","                            params['ffn_hidden_size'], params['attention_head_size'], params['layer_size'])\n","  \n","  logits = tf.keras.layers.Dense(params['len_vocab'])(decoder_outputs)\n","  predict = tf.argmax(logits, 2)\n","\n","  if PREDICT:\n","    predictions = {'indexs':predict, \n","                   'logits':logits}\n","    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n","\n","  labels_ = tf.one_hot(labels, params['len_vocab'])\n","  loss = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels_))\n","  accuracy = tf.compat.v1.metrics.accuracy(labels=labels, predictions=predict)\n","\n","  metrics = {'accuracy':accuracy}\n","  tf.summary.scalar('accuracy', accuracy[1])\n","\n","  if EVAL:\n","    return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n","  assert TRAIN\n","\n","  optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=params['learning_rate'])\n","  train_op = optimizer.minimize(loss, global_step=tf.compat.v1.train.get_global_step())\n","  return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H7PrLEWE1JCs"},"source":["### 모델 학습"]},{"cell_type":"markdown","metadata":{"id":"Gy_Opm_A7DKC"},"source":["*   필요한 각종 인자들을 설정\n","*   인자에 따라 학습 결과가 달라질 수 있기 때문에 세심한 조정이 필요\n"]},{"cell_type":"code","metadata":{"id":"CKGYuqmH6_kj","executionInfo":{"status":"ok","timestamp":1604154347293,"user_tz":-540,"elapsed":93666,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["max_len = 25\n","epoch = 5000\n","batch_size = 256\n","embedding_size = 100\n","model_hidden_size = 100\n","ffn_hidden_size = 100\n","attention_head_size = 100\n","lr = 0.001\n","layer_size = 3\n","xavier_initializer = True"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aaXalEy57ODq"},"source":["*   앞서 선언한 processing 함수로 데이터를 모델에 투입할 수 있도록 가공\n","*   평가 데이터에도 동일하게 가공"]},{"cell_type":"code","metadata":{"id":"NWlgWWIq1KSh","executionInfo":{"status":"ok","timestamp":1604154469747,"user_tz":-540,"elapsed":216108,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["import numpy as np\n","\n","train_input_enc, train_input_enc_length = enc_processing(train_input, char2idx)\n","train_output_dec, train_output_dec_length = dec_output_processing(train_label, char2idx)\n","train_target_dec = dec_target_processing(train_label, char2idx)\n","\n","eval_input_enc, eval_input_enc_length = enc_processing(eval_input, char2idx)\n","eval_output_dec, eval_output_dec_length = dec_output_processing(eval_label, char2idx)\n","eval_target_dec = dec_target_processing(eval_label, char2idx)"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qZGgZzWs7Mr7"},"source":["* 앞서 선언한 함수를 통해 모델을 선언하고 학습\n","* `tf.estimator`를 사용해 간편하게 학습 모듈 구성\n"]},{"cell_type":"code","metadata":{"id":"B9vjc3Ck7F4J","executionInfo":{"status":"ok","timestamp":1604154469750,"user_tz":-540,"elapsed":216099,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}},"outputId":"f8469601-3afc-461e-e296-153e09b8b2d4","colab":{"base_uri":"https://localhost:8080/"}},"source":["transformer = tf.estimator.Estimator(\n","    model_fn = model,\n","    params = {'embedding_size':embedding_size, \n","              'model_hidden_size':model_hidden_size,\n","              'ffn_hidden_size':ffn_hidden_size,\n","              'attention_head_size':attention_head_size,\n","              'learning_rate':lr,\n","              'len_vocab':len_vocab,\n","              'layer_size':layer_size,\n","              'max_len':max_len,\n","              'xavier_initializer':xavier_initializer}\n",")"],"execution_count":24,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using default config.\n","WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp7e9vc1ek\n","INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp7e9vc1ek', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wl_pwUiw7INZ"},"source":["* 학습한 모델을 사용해 챗봇을 사용\n","* 예측 결과를 문자열로 변환할 때는 앞서 선언한 `pred2string()` 함수를 이용\n","* 입력에 대한 응답이 생성되는 것을 확인할 수 있음\n"]},{"cell_type":"code","metadata":{"id":"COO-0PcS7Hy5","executionInfo":{"status":"ok","timestamp":1604155531646,"user_tz":-540,"elapsed":1277918,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}},"outputId":"64adaeb3-607b-4dc9-88a3-f5bc47fad2ab","colab":{"base_uri":"https://localhost:8080/"}},"source":["transformer.train(input_fn=lambda: train_input_fn(train_input_enc, train_output_dec, train_target_dec, batch_size), steps = epoch)\n","eval_result = transformer.evaluate(input_fn=lambda: eval_input_fn(eval_input_enc, eval_output_dec, eval_target_dec, batch_size))\n","print('accuracy: {accuracy: 0.3f}'.format(**eval_result))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","WARNING:tensorflow:From <ipython-input-18-c12c52eba188>:7: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n","INFO:tensorflow:Calling model_fn.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/linalg/linear_operator_lower_triangular.py:158: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Do not pass `graph_parents`.  They will  no longer be used.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n","INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp7e9vc1ek/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n","INFO:tensorflow:loss = 9.673632, step = 0\n","INFO:tensorflow:global_step/sec: 4.44085\n","INFO:tensorflow:loss = 1.5834799, step = 100 (22.522 sec)\n","INFO:tensorflow:global_step/sec: 5.13371\n","INFO:tensorflow:loss = 1.2673753, step = 200 (19.479 sec)\n","INFO:tensorflow:global_step/sec: 5.0748\n","INFO:tensorflow:loss = 1.1692473, step = 300 (19.706 sec)\n","INFO:tensorflow:global_step/sec: 4.99512\n","INFO:tensorflow:loss = 1.0421515, step = 400 (20.020 sec)\n","INFO:tensorflow:global_step/sec: 4.99569\n","INFO:tensorflow:loss = 0.88050383, step = 500 (20.017 sec)\n","INFO:tensorflow:global_step/sec: 4.93905\n","INFO:tensorflow:loss = 0.7923181, step = 600 (20.247 sec)\n","INFO:tensorflow:global_step/sec: 4.93397\n","INFO:tensorflow:loss = 0.7248307, step = 700 (20.268 sec)\n","INFO:tensorflow:global_step/sec: 4.92785\n","INFO:tensorflow:loss = 0.6221712, step = 800 (20.293 sec)\n","INFO:tensorflow:global_step/sec: 4.9239\n","INFO:tensorflow:loss = 0.47243977, step = 900 (20.306 sec)\n","INFO:tensorflow:global_step/sec: 4.88174\n","INFO:tensorflow:loss = 0.38773072, step = 1000 (20.487 sec)\n","INFO:tensorflow:global_step/sec: 4.84744\n","INFO:tensorflow:loss = 0.3206123, step = 1100 (20.629 sec)\n","INFO:tensorflow:global_step/sec: 4.83188\n","INFO:tensorflow:loss = 0.2524178, step = 1200 (20.696 sec)\n","INFO:tensorflow:global_step/sec: 4.85254\n","INFO:tensorflow:loss = 0.19773029, step = 1300 (20.607 sec)\n","INFO:tensorflow:global_step/sec: 4.83512\n","INFO:tensorflow:loss = 0.118692204, step = 1400 (20.681 sec)\n","INFO:tensorflow:global_step/sec: 4.85229\n","INFO:tensorflow:loss = 0.08952841, step = 1500 (20.610 sec)\n","INFO:tensorflow:global_step/sec: 4.83078\n","INFO:tensorflow:loss = 0.0662222, step = 1600 (20.701 sec)\n","INFO:tensorflow:global_step/sec: 4.8577\n","INFO:tensorflow:loss = 0.05134939, step = 1700 (20.586 sec)\n","INFO:tensorflow:global_step/sec: 4.85374\n","INFO:tensorflow:loss = 0.033261534, step = 1800 (20.602 sec)\n","INFO:tensorflow:global_step/sec: 4.83878\n","INFO:tensorflow:loss = 0.025117094, step = 1900 (20.668 sec)\n","INFO:tensorflow:global_step/sec: 4.84725\n","INFO:tensorflow:loss = 0.020528711, step = 2000 (20.629 sec)\n","INFO:tensorflow:global_step/sec: 4.85057\n","INFO:tensorflow:loss = 0.016794568, step = 2100 (20.619 sec)\n","INFO:tensorflow:global_step/sec: 4.83042\n","INFO:tensorflow:loss = 0.015505025, step = 2200 (20.701 sec)\n","INFO:tensorflow:global_step/sec: 4.84645\n","INFO:tensorflow:loss = 0.0112287225, step = 2300 (20.632 sec)\n","INFO:tensorflow:global_step/sec: 4.84118\n","INFO:tensorflow:loss = 0.009820211, step = 2400 (20.657 sec)\n","INFO:tensorflow:global_step/sec: 4.84855\n","INFO:tensorflow:loss = 0.30291834, step = 2500 (20.625 sec)\n","INFO:tensorflow:global_step/sec: 4.85636\n","INFO:tensorflow:loss = 0.039963387, step = 2600 (20.592 sec)\n","INFO:tensorflow:global_step/sec: 4.8502\n","INFO:tensorflow:loss = 0.014272626, step = 2700 (20.614 sec)\n","INFO:tensorflow:global_step/sec: 4.86579\n","INFO:tensorflow:loss = 0.011128246, step = 2800 (20.555 sec)\n","INFO:tensorflow:global_step/sec: 4.83755\n","INFO:tensorflow:loss = 0.008890361, step = 2900 (20.672 sec)\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2906...\n","INFO:tensorflow:Saving checkpoints for 2906 into /tmp/tmp7e9vc1ek/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2906...\n","INFO:tensorflow:global_step/sec: 4.65701\n","INFO:tensorflow:loss = 0.0068398393, step = 3000 (21.472 sec)\n","INFO:tensorflow:global_step/sec: 4.83871\n","INFO:tensorflow:loss = 0.0063946503, step = 3100 (20.666 sec)\n","INFO:tensorflow:global_step/sec: 4.84915\n","INFO:tensorflow:loss = 0.005285565, step = 3200 (20.624 sec)\n","INFO:tensorflow:global_step/sec: 4.85928\n","INFO:tensorflow:loss = 0.004932213, step = 3300 (20.578 sec)\n","INFO:tensorflow:global_step/sec: 4.82021\n","INFO:tensorflow:loss = 0.0055407714, step = 3400 (20.746 sec)\n","INFO:tensorflow:global_step/sec: 4.86015\n","INFO:tensorflow:loss = 0.0045138686, step = 3500 (20.576 sec)\n","INFO:tensorflow:global_step/sec: 4.84747\n","INFO:tensorflow:loss = 0.003224525, step = 3600 (20.628 sec)\n","INFO:tensorflow:global_step/sec: 4.84202\n","INFO:tensorflow:loss = 0.003919651, step = 3700 (20.653 sec)\n","INFO:tensorflow:global_step/sec: 4.8609\n","INFO:tensorflow:loss = 0.0028300006, step = 3800 (20.573 sec)\n","INFO:tensorflow:global_step/sec: 4.85423\n","INFO:tensorflow:loss = 0.0032383197, step = 3900 (20.600 sec)\n","INFO:tensorflow:global_step/sec: 4.86986\n","INFO:tensorflow:loss = 0.002704107, step = 4000 (20.535 sec)\n","INFO:tensorflow:global_step/sec: 4.86511\n","INFO:tensorflow:loss = 0.0032820946, step = 4100 (20.554 sec)\n","INFO:tensorflow:global_step/sec: 4.87023\n","INFO:tensorflow:loss = 0.002227013, step = 4200 (20.533 sec)\n","INFO:tensorflow:global_step/sec: 4.86331\n","INFO:tensorflow:loss = 0.002039459, step = 4300 (20.561 sec)\n","INFO:tensorflow:global_step/sec: 4.85118\n","INFO:tensorflow:loss = 0.0029567948, step = 4400 (20.614 sec)\n","INFO:tensorflow:global_step/sec: 4.85843\n","INFO:tensorflow:loss = 0.002039919, step = 4500 (20.582 sec)\n","INFO:tensorflow:global_step/sec: 4.8564\n","INFO:tensorflow:loss = 0.0018675327, step = 4600 (20.592 sec)\n","INFO:tensorflow:global_step/sec: 4.84728\n","INFO:tensorflow:loss = 0.0020663932, step = 4700 (20.629 sec)\n","INFO:tensorflow:global_step/sec: 4.84038\n","INFO:tensorflow:loss = 0.0029063844, step = 4800 (20.660 sec)\n","INFO:tensorflow:global_step/sec: 4.84075\n","INFO:tensorflow:loss = 0.0018986773, step = 4900 (20.659 sec)\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 5000...\n","INFO:tensorflow:Saving checkpoints for 5000 into /tmp/tmp7e9vc1ek/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 5000...\n","INFO:tensorflow:Loss for final step: 0.0022142925.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2020-10-31T14:45:27Z\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Inference Time : 2.93002s\n","INFO:tensorflow:Finished evaluation at 2020-10-31-14:45:29\n","INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.86578166, global_step = 5000, loss = 1.5133536\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: /tmp/tmp7e9vc1ek/model.ckpt-5000\n","accuracy:  0.866\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MNcrVf2z1LSM"},"source":["### 예측"]},{"cell_type":"markdown","metadata":{"id":"R5lY9DrW8eSK"},"source":["* 학습한 모델을 사용해 챗봇을 사용\n","* 예측 결과를 문자열로 변환할 때는 앞서 선언한 `pred2string()` 함수를 이용\n","* 입력에 대한 응답이 생성되는 것을 확인할 수 있음\n"]},{"cell_type":"code","metadata":{"id":"N9IQaBx4Qw8J","executionInfo":{"status":"ok","timestamp":1604155531651,"user_tz":-540,"elapsed":1277906,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}}},"source":["def chatbot(sentence):\n","  pred_input_enc, pred_input_enc_length = enc_processing([sentence], char2idx)\n","  pred_output_dec, pred_output_dec_length = dec_output_processing([''], char2idx)\n","  pred_target_dec = dec_target_processing([''], char2idx)\n","\n","  for i in range(max_len):\n","    if i > 0:\n","      pred_output_dec, pred_output_dec_length = dec_output_processing([answer], char2idx)\n","      pred_target_dec = dec_target_processing([answer], char2idx)\n","    \n","    predictions = transformer.predict(input_fn=lambda: eval_input_fn(pred_input_enc, pred_output_dec, pred_target_dec, 1))\n","\n","    answer, finished = pred2string(predictions, idx2char)\n","\n","    if finished:\n","      break\n","\n","  return answer"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"IjHZKvJ31MAU","executionInfo":{"status":"ok","timestamp":1604155537318,"user_tz":-540,"elapsed":1283553,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}},"outputId":"d8bc25c4-bd02-4563-cdbe-ead9ebfe225e","colab":{"base_uri":"https://localhost:8080/","height":239}},"source":["chatbot('안녕?')"],"execution_count":27,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'안녕하세요 '"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"_mjRZwyLQ_gP","executionInfo":{"status":"ok","timestamp":1604155555653,"user_tz":-540,"elapsed":1301868,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}},"outputId":"974f9e88-1f6c-461d-bcd8-776ba7b1d733","colab":{"base_uri":"https://localhost:8080/","height":545}},"source":["chatbot('너 누구냐?')"],"execution_count":28,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'좋은 방법 이란 없겠지요 '"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"T7AJCsXRTqJx","executionInfo":{"status":"ok","timestamp":1604155587281,"user_tz":-540,"elapsed":1333475,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}},"outputId":"b2a0e2cb-10ba-4abe-dc62-844b15e852dc","colab":{"base_uri":"https://localhost:8080/","height":851}},"source":["chatbot('뭐 먹었어?')"],"execution_count":29,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'저 는 배터리 가 밥 이 예요 '"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"_M8mfoUfeAWQ","executionInfo":{"status":"ok","timestamp":1604155614680,"user_tz":-540,"elapsed":1360861,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}},"outputId":"a482053f-a11c-4343-cbea-65543bd7e650","colab":{"base_uri":"https://localhost:8080/","height":647}},"source":["chatbot('놀고 싶다.')"],"execution_count":30,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'어느 것 들 이 힘들겠네요 '"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"P5mrdGRaem6v","executionInfo":{"status":"ok","timestamp":1604155640093,"user_tz":-540,"elapsed":1386261,"user":{"displayName":"조철현","photoUrl":"","userId":"00177750111141030520"}},"outputId":"4f89d315-2d86-416f-8d57-b72a7d19fab0","colab":{"base_uri":"https://localhost:8080/","height":545}},"source":["chatbot('이제 그만 잘래')"],"execution_count":31,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from /tmp/tmp7e9vc1ek/model.ckpt-5000\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'그런 날 이 있더라고요 '"]},"metadata":{"tags":[]},"execution_count":31}]}]}